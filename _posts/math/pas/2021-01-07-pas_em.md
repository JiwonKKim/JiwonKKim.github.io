---
layout: post
title:  "기댓값 최대화 알고리즘 (Expectation-Maximization Algorithm, EM Algorithm)"
category: Probability And Statistics
tags: [Probability And Statistics, Machine Learning, GMM, Gaussian, EM, EM Algorithm, 확률과 통계, 가우시안, 가우시안 혼합 모델]
comments: true
---

<br><br>
**이 글을 읽기 전에**<br>
- _해당 포스팅은 개인적인 지식, 구글링 검색 결과 등을 정리/각색하였습니다._
- _해당 포스팅은 개인적으로 공부한 내용을 정리한 것으로, 일부 내용에 오류가 있을 수 있습니다._
- _포스팅에 오류가 있을 시, 댓글로 남겨주시면 감사하겠습니다._

---

이전 포스팅에선 [GMM](https://jiwonkkim.github.io/probability%20and%20statistics/2020/12/26/pas_gmm/)에 대해서 다루어 보았습니다. 이번 포스팅에선 EM 알고리즘과 GMM의 매개변수를 EM 알고리즘을 통해 구하는 방법에 대해 다루겠습니다.<br>

EM 알고리즘은 1977년에 [Maximum Likelihood from Incomplete Data via the EM Algorithm](https://www.jstor.org/stable/pdf/2984875.pdf?casa_token=qJg81hqeCpcAAAAA:o67fWej3boU3XM_Pmnh6td9QGgNOtRdbS8fct-SuLPWKToHn5lGAbh1QzYxcxWEovgwh-V0Y9f9IFECja0um7RwXNx0qcfvkcoB6GGpPiJtpBj70ZI4)라는 논문을 통해서 정리되었습니다. EM 알고리즘의 목표는 단순히 어떤 분포 모델을 주어진 데이터에 최대한 가깝게 피팅하는 것 뿐만 아니라, 데이터의 잠재적인 표현 또한 고려하여 모델을 피팅하는 것입니다.<br>

EM 알고리즘의 장점은 레이블이 없는 비지도 학습에서 잘 동작한다는 것과, Local Maxima 혹은 적절한 성능을 내는 매개변수 지점의 탐색을 보장한다는 것입니다.<br>

EM 알고리즘의 주요 목표는 데이터의 유용한 특징(Feature)을 잡아내는 잠재적인 표현을 계산하는 것입니다. 이 잠재적인 표현, GMM에서는 잠재변수라고 볼 수 있는 독특한 표현은, 모델링한 통계 모델에 대한 이해를 향상시키는 데 도움이 되고, 이는 잠재 공간의 표현을 다시 계산하는데 도움이 됩니다.<br>

EM 알고리즘은 통계 모델을 개선하고, 데이터의 잠재 표현을 업데이트하는 두 가지 행위를 수렴 조건이 충족될 때까지 번갈아가며 수행합니다. 알고리즘의 설명에 앞서, 사용되는 기호들과 이에 대한 설명입니다.

<ul>
  <li>$\boldsymbol{X}$: Raw Data set입니다. (i.e. 이미지의 픽셀값 등)</li>
  <li>$p(\boldsymbol{X}|\boldsymbol{Z},\boldsymbol{\theta})$: 매개변수가 $\boldsymbol{\theta}$, 잠재변수가 $\boldsymbol{Z}$로 주어져있는 모델 하에서 보는 $\boldsymbol{X}$의 가능도입니다.</li>
  <li>$p(\boldsymbol{Z}=k)$: 잠재변수 Z의 사전 확률, 또는 무작위로 샘플링 된 데이터가 특정 잠재 영역에 포함될 확률입니다. GMM에서 보면 $k$라는 가우시안 함수의 구성 성분인 데이터를 관측할 확률과 같습니다.</li>
  <li>$\boldsymbol{\theta}$: 모델의 매개변수입니다. GMM에서 보면 $\Sigma$, $\mu$, $\pi$에 해당합니다.</li>
</ul>

<br>
자, 우리가 지금 하고자 하는 것은 $\boldsymbol{p(X,Z|\theta)}$를 구하고, 이를 통해 $\boldsymbol{p(X|\theta)}$를 구하는 것이 목표입니다. 이를 계속 머릿속에 염두해두며 글을 읽어주시면 됩니다.

EM 알고리즘은 총 세 단계로 이루어져 있고, 2단계와 3단계를 수렴할 때까지 반복합니다 (4단계를 수렴이라고 보시면 되겠습니다.). 각 단계에 대해서 설명하겠습니다.<br>
##### 1단계: Initialization Step
1단계에서는 매개변수 $\boldsymbol{\theta}$를 초기화합니다. 초기화는 랜덤하게, 아니면 **K-means(Will be posted later)**를 사용합니다.<br>

##### 2단계: Expectation Step (E-Step)
E-Step은 잠재변수 $\boldsymbol{Z}$를 업데이트하고, 매개변수 $\boldsymbol{\theta}$를 업데이트하기 위한 수단으로 사용됩니다. E-Step 또한 두 단계로 나뉠 수 있는데, 첫 번째 단계는 주어진 데이터의 잠재 영역 할당을 표현하는 조건부 확률 $p(\boldsymbol{Z}|\boldsymbol{X},\boldsymbol{\theta})$를 새로 업데이트된 $\boldsymbol{\theta}$를 통해 업데이트 하는 것입니다. $p(\boldsymbol{Z}|\boldsymbol{X},\boldsymbol{\theta})$는 하단의 식 (2)와 같이 표현될 수 있습니다.<br>
<p>(Bayes's Theorem) $p(Z|X,\theta)=\frac{p(X|\theta,Z)p(Z)}{p(X,\theta)}\tag{1}$</p>
<p>(Bayes's Theorem) $p(z_{i}=k|x_{i},\theta)=\frac{p(x_{i}|\theta,z_{i}=k)p(z_{i}=k)}{\sum_{j=1}^k p(x_{i}|z_{i}=j,\theta)p(z_{i}=j)}\tag{2}$</p>

<br>
두 번째 단계는 $\boldsymbol{Q(\theta,\theta^\*)}$를 업데이트 하는 것입니다. 여기서 $\boldsymbol{\theta^\*}$는 모델이 매개변수를 업데이트하기 직전의 매개변수, $\boldsymbol{\theta}$는 업데이트될 새로운 매개변수를 의미합니다. 이제 $\boldsymbol{Q(\theta,\theta^\*)}$가 무엇인지와, $\boldsymbol{Q(\theta,\theta^\*)}$를 도출하는 과정을 보여드리겠습니다.<br>

<p>(Complete log-likeihood) $\log \boldsymbol{p(X|\theta)} = \sum_{i=1}^{n} \log \boldsymbol{p(x_{i}|\theta)}  = \sum_{i=1}^n \log \sum_{j=1}^{k} \boldsymbol{p(x_{i},z_{j}|\theta)}\tag{3}$</p>

<br>
먼저 우리가 구하고자 하는 대상은 $\boldsymbol{p(X,Z|\theta)}$입니다. 이를 식(3)과 같이 표현할 수 있습니다. 여기서 식 (3)은 $\boldsymbol{X}$, $\boldsymbol{Z}$ 두 변수에 대한 Likelihood를 구한것이 되므로 이를 **Complete log-likelihood**라고 합니다. 이 Complete log-likelihood에 $\boldsymbol{Z|X,\theta^{*}}$에 대해 **기댓값(Expectation)**을 취해준 값이 바로 $\boldsymbol{Q(\theta,\theta^\*)}$입니다. 그런데 일반적으로 우리는 잠재변수 $\boldsymbol{Z}$에 대한 정보가 없으므로 이 식을 Posterior Distribution으로 다시 표현해야 합니다. 따라서 우리는 데이터 $x_{i}$에 해당하는 잠재변수 $z_{i}$가 일대일대응으로 존재한다고 가정하고 새로운 식 (4)를 만들겠습니다. 식 (4)는 하기와 같이 표현할 수 있습니다.<br>

<p>(Bayes's Theorem) $\sum_{i=1}^n \log{\boldsymbol{p(x_{i},z_{i}|\theta)}} = \sum_{i=1}^n \log{\boldsymbol{p(z_{i}|\theta)p(x_{i}|z_{i},\theta)}}\tag{4}$</p>

<br>
하지만 식 (4)와 같이 표현하면 $z_{i}$가 어떤 변수인지 모르게 되므로, 식 (4)를 $\boldsymbol{Z|X,\theta^{*}}$에 대해 기댓값을 취함으로써 $z_{i}$를 추측합니다. **(잘 이해가 잘 안되는 부분입니다. 개인적으로는 우리에게 데이터 $\boldsymbol{x}$가 주어져 있으니, 모든 데이터들의 분포를 평균화 하여 $z$를 추측하는 것으로 보입니다.)** 기댓값은 하단의 식 (5), (6)과 같이 표현할 수 있습니다.<br>

<p>$\boldsymbol{Q(\theta,\theta^*)} = \mathbb{E}_{\boldsymbol{Z|X,\theta^{*}}}[\sum_{i=1}^n \log \boldsymbol{p(z_{i}|\theta)p(x_{i}|z_{i},\theta)}]\tag{5}$</p>
<p>$\boldsymbol{Q(\theta,\theta^*)} = \sum_{i=1}^n \sum_{j=1}^k \boldsymbol{p(z_{i}=j|X,\theta^{*})} \log [\boldsymbol{p(z_{i}=j|\theta)p(x_{i}|z_{i}=j,\theta)}]\tag{6}$</p>

<br>
식 (6)에서 $\boldsymbol{p(z_{i}=j|X,\theta^{*})}$는 위에서 언급한 주어진 데이터의 잠재 영역 할당을 표현하는 조건부 확률, $\boldsymbol{p(z_{i}|\theta)}$는 잠재변수 $\boldsymbol{Z}$의 사전확률, $\boldsymbol{p(x_{i}|z_{i},\theta)}$은 잠재변수 $\boldsymbol{Z}$가 주어졌을때 우리 데이터의 분포확률이 되겠네요.

<br>
우리가 잠재변수 $\boldsymbol{Z}$에 대해 알고 있다면, $\boldsymbol{p(X,Z|\theta)}$를 잠재변수 $\boldsymbol{Z}$에 대해 Marginalization하여 $\boldsymbol{p(X|\theta)}$를 구할 수 있습니다. 하지만 이러한 Marginalization 자체가 까다로운 작업일 뿐만 아니라, 잠재변수 $\boldsymbol{Z}$가 미지의 변수이기 때문에, 우리가 최대화하고자 하는 $\boldsymbol{X}$와 $\boldsymbol{Z}$의 Complete log-likelihood를 $\boldsymbol{Q(\theta,\theta^*)}$로 표현하는 것입니다.<br>

##### 3단계: Maximization Step (M-Step)
M-Step은 간단합니다. 매개변수 $\theta$를 업데이트하는 단계이며, 계산한 $\boldsymbol{Q(\theta,\theta^*)}$의 최댓값을 뽑아내는 $\theta$값을 구한다.
<p>$$\theta^{*} = argmax_{\theta} Q(\theta, \theta^{*})\tag{7}$$</p>

##### 4단계: Convergence Step
M-Step에서 구한 $\boldsymbol{Q^{new}(\theta,\theta^\*)}$와 이전 M-Step에서 구한 $\boldsymbol{Q^{old}(\theta,\theta^\*)}$를 비교하여, 새로 구한 $\boldsymbol{Q^{new}(\theta,\theta^\*)}$가 EM 과정을 반복하여도 값의 변화가 크다고 판단되지 않을 경우 알고리즘을 종료시킵니다.<br>

<br>
이제 위의 내용들을 바탕으로 최적화된 GMM의 매개변수를 구해보도록 하겠습니다.<br>

먼저, [지난 GMM 포스팅](https://jiwonkkim.github.io/probability%20and%20statistics/2020/12/26/pas_gmm/)에 이어 어떤 한 개의 데이터 $x_{n}$가 $k$라는 가우시안 함수의 구성성분일 확률을 재정의 해보겠습니다.
<p>(Bayes's Theorem) $p(z_{k}=1|\boldsymbol{x}_{n}) = \frac{p(\boldsymbol{x}_{n}|z_{k} = 1)p(z_{k} = 1)}{\sum_{j=1}^K p(\boldsymbol{x}_{n}|z_{j} = 1)(p(z_{j} = 1))} \tag{8}$</p>

이어서, 가우시안 함수의 크기를 의미하는 혼합 계수를 이용해, 어떤 데이터가 $k$라는 가우시안 함수에서 관측될 확률이 주어졌을 때 그것이 우리에게 주어진 데이터일 확률을 재정의합니다.
<p>$$p(z_{k} = 1) = \pi_{k}, \qquad p(\boldsymbol{x}_{n}|z_{k} = 1) = \mathcal{N}(\boldsymbol{x}_{n}|\mu_{k}, \Sigma_{k})\tag{9}$$</p>

식 (8)과 식 (9)를 이용해서 어떤 한 개의 데이터 $x_{n}$가 $k$라는 가우시안 함수의 구성성분일 확률을 ${\gamma}(z_{nk})$로 간략히 표현하겠습니다.
<p>$${\gamma}(z_{nk}) = p(z_{k}=1|\boldsymbol{x}_{n}) = \frac{\pi_{k} \mathcal{N}(\boldsymbol{x}_{n}|\mu_{k}, \Sigma_{k})}{\sum_{j=1}^K \pi_{j} \mathcal{N}(\boldsymbol{x}_{n}|\mu_{j}, \Sigma_{j})} \tag{10}$$</p>

그리고, GMM에서 사용되는 매개변수는 $\theta = \{ \pi, \mu, \Sigma \}$로 구성되어 있습니다.<br>

자, 이제 EM 알고리즘을 단계벌로 적용해봅시다.<br>
##### 1단계: Initialization Step
먼저 매개변수 $\theta\$를 초기화합니다. 위에서 언급했던 것처럼, 랜덤으로 초기화하여도 되고, K-means를 사용하여도 좋습니다.<br>

##### 2단계: Expectation Step (E-Step)
우리는 잠재변수 $\boldsymbol{Z}$를 모델링 했기 때문에 $\boldsymbol{X}$와 $\boldsymbol{Z}$의 Complete log-likelihood를 구할 수 있으므로, $Q(\theta,\theta^\*)$를 구하는데 Complete log-likelihood의 Posterior Distribution 대신 Complete log-likelihood를 사용하도록 하겠습니다. 그리고, 모든 데이터에 대한 고려한 식 (6)에서 등장한 $\boldsymbol{Q(\theta,\theta^*)}$의 형태를 간단하게 바꾸어 재정의하겠습니다. 여기서 $\boldsymbol{X}$는 모든 데이터 집합을 의미합니다.
<p>$$Q(\theta,\theta^*) = \mathbb{E}[\log p(\boldsymbol{X},\boldsymbol{Z}|\theta)] = \sum_{\boldsymbol{Z}} p(\boldsymbol{Z}|\boldsymbol{X},\theta^*) \log p(\boldsymbol{X},\boldsymbol{Z}|\theta) \tag{11}$$</p>

이제 식 (11)에 식 (10)을 대입합니다.
<p>$$Q(\theta,\theta^*) = \sum_{\boldsymbol{Z}} \gamma(z_{nk}) \log p(\boldsymbol{X},\boldsymbol{Z}|\theta) \tag{12}$$</p>

이제 Complete log-likelihood를 구해봅시다.
<p>$$p(\boldsymbol{X},\boldsymbol{Z}|\theta) = p(\boldsymbol{X}|\boldsymbol{Z}|\theta)p(\boldsymbol{Z},\theta) = \prod_{n=1}^{N} \prod_{k=1}^K \pi^{z_{nk}} \mathcal{N}(\boldsymbol{x_{n}}|\mu_{k}, \Sigma_{k})^{z_{nk}} \tag{13}$$</p>
<p>$$\log p(\boldsymbol{X},\boldsymbol{Z}|\theta) = \sum_{n=1}^{N} \sum_{k=1}^K z_{nk}[\log{\pi_{k}} + \log{\mathcal{N}(\boldsymbol{x_{n}}|\mu_{k}, \Sigma_{k})}] \tag{14}$$</p>

이렇게 하여, E-Step에서 구하고자 하는 $Q(\theta,\theta^*)$의 식은 다음과 같습니다.
<p>$$Q(\theta,\theta^*) = \sum_{n=1}^{N} \sum_{k=1}^K z_{nk}[\log{\pi_{k}} + \log{\mathcal{N}(\boldsymbol{x_{n}}|\mu_{k}, \Sigma_{k})}] \tag{15}$$</p>

##### 3단계: Maximization Step (M-Step)
이제, $Q(\theta,\theta^\*$를 최대화하기 위하여 식 (15)에 **라그랑즈 승수법(Lagrange Multiplier Method, Will be posted later)**를 적용해 보겠습니다. GMM에 걸려진 제약조건은 $\sum_{k=1}^K \pi_{k}=1$이므로, 이를 식 (15)에 적용하면 다음과 같습니다.
<p>$$Q(\theta,\theta^*) = \sum_{n=1}^{N} \sum_{k=1}^K z_{nk}[\log{\pi_{k}} + \log{\mathcal{N}(\boldsymbol{x_{n}}|\mu_{k}, \Sigma_{k})}] - \lambda(\sum_{k=1}^K \pi_{k} - 1) \tag{16}$$</p>

매개변수를 최대화하기 위해, $Q(\theta,\theta^\*)$를 각각의 매개변수 $\{ \pi, \mu, \Sigma \}$에 대해 편미분하여 0이 되는 매개변수를 찾습니다.
<p>$$\frac{\partial{Q(\theta,\theta^*)}}{\partial{\pi_{k}}} = \sum_{n=1}^{N} \frac{\gamma(z_{nk})}{\pi_{k}} - \lambda = 0 \tag{17}$$</p>
<p>$$\sum_{n=1}^N \gamma(z_{nk}) = \pi_{k} \lambda \quad \Rightarrow \quad \sum_{k=1}^{K} \sum_{n=1}^{N} \gamma(z_{nk}) = \sum_{k=1}^K \pi_{k} \lambda \tag{18}$$</p>
<p>$$\sum_{k=1}^K \gamma(z_{nk}) = 1, \sum_{k=1}^K \pi_{k} = 1 \quad \Rightarrow \quad \lambda = N \tag{19}$$</p>
<p>$$\pi_{k} = \frac{\sum_{n=1}^N \gamma(z_{nk})}{N} \tag{20}$$</p>

이와 마찬가지로, $\mu_{k}$와 $\Sigma_{k}$를 구할 수 있습니다. 계산식은 생략하고, 결과만 보여드리겠습니다.
<p>$$\mu_{k} = \frac{\sum_{n=1}^N \gamma(z_{nk}) \boldsymbol{x_{n}}}{\sum_{n=1}^N \gamma(z_{nk})} \tag{21}$$</p>
<p>$$\Sigma_{k} = \frac{\sum_{n=1}^N \gamma(z_{nk}) (\boldsymbol{x_{n}}-\mu_{k}) (\boldsymbol{x_{n}}-\mu_{k})^{T}}{\sum_{n=1}^N \gamma(z_{nk})} \tag{22}$$</p>

##### 4단계: Convergence Step
이제 2, 3단계를 매개변수가 수렴할 때까지 반복하시면 됩니다.<br>

이렇게 EM 알고리즘과, 이를 통해서 최적화된 GMM의 매개변수를 구하는 방법에 대해서 살펴보았습니다. 제가 정확히 알고 있지 않은 지식에 대해서 이것저것 찾아보고 정리하는 과정이 너무 어렵네요...<br>
이번 포스팅에선 EM 알고리즘을 의미적으로 살펴보았는데, 다음에 기회가 되면 이것을 좀 더 수식적인 의미를 포함하여 포스팅을 해보도록 하겠습니다.<br>

#### 이번 포스팅 정리
- EM 알고리즘의 목표는 단순히 어떤 분포 모델을 주어진 데이터에 최대한 가깝게 피팅하는 것 뿐만 아니라, 데이터의 잠재적인 표현 또한 고려하여 모델을 피팅한다.
- EM 알고리즘의 장점은 레이블이 없는 비지도 학습에서 잘 동작한다는 것과, Local Maxima 혹은 적절한 성능을 내는 매개변수 지점의 탐색을 보장한다. (물론, Local Maxima에 빠진다는 점에서 단점이 될 수 있다.)
- EM 알고리즘은 Initialization - Expectaiton - Maximization - Convergence 총 4가지 Step으로 이루어져 있으며, 이 중 E와 M Step을 수렴할때까지 반복한다.
- Expectation을 취해주는 이유는 전체적인 데이터를 통해서 잠재변수 $Z$를 추정하기 위함이라고 생각된다. (잘 모르겠음...)
- 일반적으로 잠재변수 $Z$는 알려지지 않은 값이므로, $X$와 $Z$의 Complete log-likelihood를 Posterior Distribution으로 다시 표현하여 $Q(\theta,\theta^\*)$를 정의한다.

##### 출처:
- [Maximum Likelihood from Incomplete Data via the EM Algorithm](https://www.jstor.org/stable/pdf/2984875.pdf?casa_token=qJg81hqeCpcAAAAA:o67fWej3boU3XM_Pmnh6td9QGgNOtRdbS8fct-SuLPWKToHn5lGAbh1QzYxcxWEovgwh-V0Y9f9IFECja0um7RwXNx0qcfvkcoB6GGpPiJtpBj70ZI4)
- [TOWARDS DATASCIENCE - EM](https://towardsdatascience.com/solving-a-chicken-and-egg-problem-expectation-maximization-em-c717547c3be2)
- [TOWARDS DATASCIENCE - GMM](https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95)
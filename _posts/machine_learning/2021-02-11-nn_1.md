---
layout: post
title: 신경망(Neural Networks) (1)
category: Machine Learning
tags: [Machine Learning, Andrew Ng, Neural Networks, 신경망, Forward Propagation, 순전파]
comments: true
---

<br><br>
**이 글을 읽기 전에**<br>
- _해당 포스팅은 Coursera에 올라와 있는 Andrew Ng 교수님의 Machine Learning 강좌를 정리/각색하였습니다._
- _해당 포스팅은 개인적으로 공부한 내용을 정리한 것으로, 일부 내용에 오류가 있을 수 있습니다._
- _포스팅에 오류가 있을 시, 댓글로 남겨주시면 감사하겠습니다._

---
 
꼬박꼬박 포스팅하는게 참 어렵습니다. 그래도 열심히 해보도록 하겠습니다.

이번 포스팅에서는 **신경망(Neural Networks)**에 대해 알아보도록 하겠습니다.

현 시점을 기준으로 기계학습에서 가장 많이 적용되는 기술인 신경망이 등장했습니다. 그런데 사실 신경망은 꽤 오래전에 등장한 방법이지만, 사람들의 관심을 끌지는 못했습니다. 이미 선형 회귀나 로지스틱 회귀와 같은 방법들을 기계학습에 적용하여 다양한 문제를 풀 수 있는데, 왜 신경망이 필요할까요? 이를 알기 위해서 먼저 비선형 가설함수를 사용하는 기계학습 예제를 살펴보겠습니다.

<center><img src="/assets/ml/09_neural_network_1/Fig01_non-linear_hyp.png" width="700" height="350"></center>
<center>Fig. 1. Non-linear Hypothesis with Car Image</center>

<br>
지금까지 우리가 본 기계학습 예제들은 Feature의 개수가 적었습니다. 하지만 실제로 기계학습에서 사용되는 Feature의 개수는 굉장히 많습니다. Fig. 1 과 같이 $50 \times 50$ 의 크기를 가진 이미지가 차인지 차가 아닌지 구분하는 문제를 봅시다. 각각의 픽셀값은 이미지의 밝기를 의미합니다. 이를 바탕으로 각각의 픽셀값을 통해 차인지 아닌지 구분하는 모델을 설계한다고 합시다. 예를 들어, 픽셀 1과 픽셀 2에 해당하는 픽셀값과, 이를 통해 차인지 아닌지 구분한 결과는 Fig. 1 의 좌하단 그래프와 같습니다. 하지만 이렇게 모델을 구성한다면, 우리는 $50 \times 50$ 개의 픽셀, 즉 2500개의 Feature를 사용하여 모델링을 해야 합니다. 하지만, 단순히 2500개의 Feature만을 사용하여 선형 함수로 모델링 한다면, Decision Boundary가 Fig. 1 의 좌하단 그래프와 같은 형태를 가지기 어려울 것입니다. 따라서 2500개의 픽셀을 2차식 Feature가 되도록 재구성 한다면 Feature의 개수는 약 300만개가 될 것이고, 이렇게 많은 Feature를 사용하는 것은 계산이 오래 걸리게 됩니다.

이에 반면, 신경망은 Feature의 개수를 적정선에서 선택하여도 충분한 성능을 발휘할 수 있습니다.

<center><img src="/assets/ml/09_neural_network_1/Fig02_neuron.png" width="500" height="250"></center>
<center>Fig. 2. Neuron in the Brain</center>

<br>
신경망은 뇌속의 뉴런(Neuron)이나 뉴런의 연결망처럼 보이게 만들어졌습니다. 뉴런은 두뇌의 세포이고, 우리의 뇌는 Fig. 2 와 같은 뉴런으로 구성되어 있습니다. 뉴런은 Fig. 2 와 같이 수상돌기(Dendrite)라 불리는 수많은 입력 단자를 가지고 있는데, 우리가 기계학습에서 보던 데이터 입력을 이 수상돌기, 즉 입력 단자라고 보시면 됩니다. 그리고 뉴런은 축삭돌기(Axon)이라고 불리는 출력 단자를 가지고 있는데, 이는 다른 뉴런으로 신호를 전달하는데 사용됩니다.

간단히 정리해보면 뉴런은 계산하는 유닛이고, 여러개의 입력 단자로부터 값을 받아 특정 계산을 수행하고, 이의 출력값을 다른 뉴런으로 전달해줍니다. 우리가 몸을 움직이려고 한다면, 뉴런은 근육에 전기 신호를 보내어 근육을 수축시키고, 눈과 같은 감각이 두뇌로 메세지를 보면 동일한 방식으로 감지합니다. 신경망의 Layer 내부에 있는 유닛은 뉴런과 같은 역할을 합니다.

<center><img src="/assets/ml/09_neural_network_1/Fig03_neuron_network.png" width="500" height="300"></center>
<center>Fig. 3. Neuron Network</center>

<br>
Fig. 3 은 신경망을 나타내는 그림입니다. 노란색 노드는 뉴런을 의미하며, 노드로 들어오는 Wire를 Input Wire, 즉 수상돌기로, 노드에서 나가는 Wire를 Output Wire, 즉 축삭돌기로 보시면 됩니다. 그림처럼 출력이 항상 1인 $x_0$ , $a_0^{(2)}$ 를 추가하여 각 Layer의 편향(Bias)를 직관적으로 나타낼 수도 있습니다. 신경망의 첫 번째 Layer는 Input Feature를 받기 때문에 Input Layer라고 불리우며, 마지막 Layer는 가설을 계산한 최종 값을 출력하기 때문에 Output Layer라고 불립니다. 그리고 그 중간에 있는 Layer들은 훈련 데이터에서 볼 수 없기 때문에, $x$ 도 $y$ 도 아니라서 Hidden Layer라고 불립니다. 즉, Input Layer와 Output Layer가 아닌 것들은 다 Hidden Layer라고 보시면 됩니다.

Fig. 3 의 $a_{i}^{(j)}$ 는 Layer $j$ 에 있는 $i$ 번째 뉴런의 Activation입니다. 여기서 Activation은 유닛에서 계산된 Output값을 의미합니다. 그리고 이 Activation을 계산할 때 매개변수 $\boldsymbol{\Theta}^{j}$ 는 Layer $j$ 에서 Layer $j+1$ 로 입력 값을 Mapping할 때 사용됩니다. 즉, Fig. 3 을 식으로 표현하면 다음과 같습니다.

<p>$$\eqalign{
a_{1}^{(2)} &=  g({\Theta}_{10}^{(1)}x_{0} + {\Theta}_{11}^{(1)}x_{1} + {\Theta}_{12}^{(1)}x_{2} + {\Theta}_{13}^{(1)}x_{3}) \\
a_{2}^{(2)} &=  g({\Theta}_{20}^{(1)}x_{0} + {\Theta}_{21}^{(1)}x_{1} + {\Theta}_{22}^{(1)}x_{2} + {\Theta}_{23}^{(1)}x_{3}) \\
a_{3}^{(2)} &=  g({\Theta}_{30}^{(1)}x_{0} + {\Theta}_{31}^{(1)}x_{1} + {\Theta}_{32}^{(1)}x_{2} + {\Theta}_{33}^{(1)}x_{3}) \\
h_{\boldsymbol{\Theta}}(x) &= a_{1}^{(3)} = g({\Theta}_{10}^{(2)}a_{0}^{2} + {\Theta}_{11}^{(2)}a_{1}^{2} + {\Theta}_{12}^{(2)}a_{2}^{2} + {\Theta}_{13}^{(2)}a_{3}^{2})
\tag {1}
}$$</p>

여기서 $g$ 는 Sigmoid Function입니다. 신경망에서는 한 Layer에서 계산을 하고 난 다음 Activation, 즉 활성화를 해주는 것이 보편적이기 때문에 **활성화 함수(Activation Function)(Will be posted Later)**중 하나인 Sigmoid Function이 자연스럽게 식 (1)에 포함되어 있습니다. 이 때 활성화 함수는 꼭 Sigmoid Function이 아니어도 됩니다. 활성화 함수가 무엇인지는 당장 이해하지 않으셔도 됩니다. 일단 $g$ 가 의미하는 것이 활성화 함수라는 것만 알아두시면 되겠습니다.

Fig. 3 의 신경망에서 Input Layer Unit은 편향을 제외하고 3개, Hidden Layer Unit는 3개입니다. 따라서 이 2개의 Layer를 이어주는, 즉 Input Layer에서 Hidden Layer로 Mapping할 때 사용되는 매개변수 $\boldsymbol{\Theta}^{j}$ 는 $3 \times 4$ 차원 행렬이 됩니다 ($\boldsymbol{\Theta}^{j} \in \mathbb{R}^{3 \times 4}$). 즉, 신경망 네트워크가 Layer $j$ 에 $s_j$ 개의 유닛이 있고, Layer $j+1$ 에 $s_{j+1}$ 개의 유닛이 있다면, 매개변수 $\boldsymbol{\Theta}^{j}$ 의 차원은 $s_{j+1} \times (s_{j}+1)$ 가 됩니다. Layer $j+1$ 의 Activation 계산에는 Layer $j$ 의 편향이 포함되어야 하니까요.

그런데 식 (1)을 매번 표현하기에는 너무 복잡하니, 이를 간단히 정리해보겠습니다.

식 (1)의 활성화 함수 안에 있는 항을 $z_{i}^{(j)}$ 로 표현하겠습니다. 즉,
$\Theta_{10}^{(1)}x_{0} + \Theta_{11}^{(1)}x_{1} + \Theta_{12}^{(1)}x_{2} + \Theta_{13}^{(1)}x_{3}$ 는 $z_{1}^{(2)}$ 와 같이 표현되고, $a_{1}^{(2)} = g(z_{1}^{(2)})$ 가 됩니다. 이를 사용하여 식 (1)을 벡터화 하면 다음과 같습니다.
<p>$$\boldsymbol{x} = \begin{bmatrix} x_0 \\ x_1 \\ x_2 \\ x_3 \end{bmatrix} = \boldsymbol{a}^{(1)}, \boldsymbol{z}^{(2)} = \begin{bmatrix} z_{1}^{2} \\ z_{2}^{2} \\ z_{3}^{2} \end{bmatrix} \tag{2}$$</p>
<p>$$\boldsymbol{z}^{(2)} = \boldsymbol{\Theta}^{1} \boldsymbol{x} = \boldsymbol{\Theta}^{1} \boldsymbol{a}^{(1)} \tag{3}$$</p>
<p>$$\boldsymbol{a}^{(2)} = g(\boldsymbol{z}^{(2)})\tag{4}$$</p>

하지만 여기서 식 (4)와 같이 $\boldsymbol{a}^{(2)}$ 를 계산하면 가설을 계산할 때 Hidden Layer의 편향이 빠지게 됩니다. 이를 방지하기 위해서 $\boldsymbol{a}^{(2)}$ 에 $a_{0}^{(2)} = 1$ 항을 따로 추가하여 계산합니다.

<p>$$\boldsymbol{z}^{(3)} = \boldsymbol{\Theta}^{(2)} \boldsymbol{a}^{(2)} \tag{5}$$</p>
<p>$$h_{\boldsymbol{\Theta}} = \boldsymbol{a}^{(3)} = g(\boldsymbol{z}^{(3)}) \tag{6}$$</p>

이렇게 Input Layer부터 시작하여 Hidden Layer를 거쳐 Output Layer에서 가설을 계산하는 과정까지를 **순전파(Forward Propagation)**라고 합니다.

<center><img src="/assets/ml/09_neural_network_1/Fig04_covered_nn.png" width="500" height="250"></center>
<center>Fig. 4. Covered Neuron Network</center>

<br>
다시 돌아와서, 왜 기계학습에서 신경망이 필요한지 생각해보겠습니다. Fig. 4 는 Fig. 3 에서 Input Layer를 제거한 것입니다. 이 그림은 로지스틱 회귀와 똑같은 형태를 띄고 있습니다. **즉, 이 신경망은 로지스틱 회귀와 같지만, 기존의 훈련 데이터셋인 $\boldsymbol{x}$ 를 새로운 Feature인 $\boldsymbol{a}^{(2)}$ 로 바꾸어 입력으로 사용합니다.** 이러한 방법의 장점은 $\boldsymbol{a}^{(2)}$ 가 입력의 함수로써 새로운 Feature로 학습된다는 것입니다. 새로운 Feature를 만듦으로써 기계학습에 Flexibility가 생기고, 기존의 Feature만을 사용하는 한계를 극복할 수 있습니다.

#### 이번 포스팅 정리
- 신경망은 Feature의 개수가 너무 많은 다항식 회귀와 달리 Feature의 개수를 적정선에서 선택하여도 충분한 성능을 발휘할 수 있다.
- 신경망의 유닛은 계산하는 유닛이고, 여러개의 입력 단자로부터 값을 받아 특정 계산을 수행하고, 이의 출력값을 다른 유닛으로 전달한다.
- 신경망은 입력부의 Input Layer, 출력부의 Output Layer, 그리고 이 둘을 제외한 나머지 모든 Layer인 Hidden Layer로 구성되어 있다.
- Input Layer부터 시작하여 Hidden Layer를 거쳐 Output Layer에서 가설을 계산하는 과정까지를 순전파라고 한다.
- 신경망은 기존에 우리가 보던 회귀와 같지만, 기존의 훈련 데이터셋인 $\boldsymbol{x}$ 를 새로운 Feature로 바꾸어 입력으로 사용한다.
- 이러한 방법의 장점은 입력된 Feature가 새로운 Feature로 학습된다는 것이다. Feature를 만듦으로써 기계학습에 Flexibility가 생기고, 기존의 Feature만을 사용하는 한계를 극복할 수 있다.

##### 출처:
- [Andrew Ng - Machine Learning](https://www.coursera.org/learn/machine-learning)
